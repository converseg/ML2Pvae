---
title: "ML2P-VAE"
author: "Geoffrey Converse"
header-includes:
   - \usepackage{amsmath, amsfonts, amssymb, amsthm, mathtools, graphicx, multicol, float}
output: rmarkdown::pdf_document
vignette: >
  %\VignetteIndexEntry{ml2p_vae_vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup}
library(ML2Pvae)
```
\newtheorem{thm}{Theorem}
\def \R{\ensuremath \mathbb{R}}
\def \e{\ensuremath \varepsilon}
\def \d{\ensuremath \delta}
\def \tr{\ensuremath \text{tr}}

# Introduction
This package allows easy implementation of the ML2P-VAE method for estimating parameters in Item Response Theory (IRT). This method was first proposed by Curi et. al. \cite{curi2019}, and was later compared with a similar method which used regular autoencoders, rather than variational autoencoders (VAE) \cite{ae_v_vae}. Other parameter estimation methods rely on numerical integration or MCMC methods, and are infeasible when the latent ability dimension is greater than 8. ML2P-VAE methods learn one function mapping response sets to latent abilities, and another function mapping latent abilities to the probability of answering items correctly. The latter is the Multidimensional Logistic 2-Parameter model.


## Multidimensional Logistic 2-Parameter (ML2P) Model
In Item Response Theory, the goal is often to quantify the relation between the underlying ability (or abilities) of students and their performance on an assessment. Formally, we assume that every student $j$ posesses some latent ability value $\Theta_j$, which is not directly observable. Often, we assume that $\Theta_j$ is distributed normally across the population of $N$ students. In a practical setting, our data for each student may be a binary vector $u_j \in \R^n$ where a 1 (resp. 0) corresponds to a correct (resp. incorrect) answer, to an exam consisting of $n$ questions (items). Given a student $j$'s assessment results, how can we infer the latent value $\Theta_j$?

A naive approach is to simply use the raw exam score, or accuracy. But if two students both score 80\% on an exam while missing different items (so $u_1 - u_2 \not= 0_n$), it is unlikely that they have the exact same ability value. Not all exam items are equal - they often vary in difficulty and in the skills required to answer them correctly.

Instead, theory has been developed \cite{thissen} that states that the probability of student $j$ answering item $i$ correctly is a function of $\Theta_j$, along with parameters $\Lambda_i$ associated with item $i$:
\[P(u_{ij} = 1 | \Theta_j) = f(\Theta_j; \Lambda_i)\]
In this work, we are concerned with the case where $\Theta_j = (\theta_{j1}, ..., \theta_{jK})^\top \in \R^K$, so that an exam is designed to assess multiple skills. For example, an elementary math exam may test the abilities "add," "subtract," "multiply," and "divide." In order to keep track of which items asses which abilities, we can develop a binary matrix $Q \in \R^{K \times n}$ where
\[Q_{ki} = \begin{cases}
    1 & \text{if item } i \text{ requires skill } k \\ 
    0 & \text{otherwise} 
    \end{cases}\]
It is important to note that multiple items can require multiple skills.

The model that this package focuses on is the Multidimensional Logistic 2-Parameter (ML2P) model. The probability of a correct response is given by
\[P(u_{ij}=1 | \Theta_j) = \frac{1}{1 + \exp[-\vec a_i^\top\Theta_j + b_i]} = \frac{1}{1 + \exp[-\sum_{k=1}^K a_{ik}\theta_{jk} + b_i]}\]
where the discrimination parameters $\vec a_i^\top = (a_{ik})_{1\leq k\leq K}$ determine \textit{how much} of skill $k$ is required to answer item $i$ correctly, and the difficulty parameter $b_i$ quantifies the difficulty of item $i$.

In application, we often only have response vectors for a set of students - so we need to estimate the ability parameters $\Theta_j$ for each student, along with the discrimination and difficulty parameters for each item. While there are many parameter estimation methods, including the Expectation-Maximization algorithm and various MCMC methods, many of them become infeasible as the number of latent traits increases beyond $\dim(\Theta_j) = 7$.


# ML2P-VAE Description
Certain modifications to the typical VAE architecture \cite{kingma} are required in order to be used as parameter estimation methods. First, there are no hidden layers in the decoder. Instead, the non-zero weights in the decoder, are determined by a given $Q$-matrix. These connect the encoded distribution layer directly to the output layer. The output layer must use the sigmoidal activation function,
\begin{equation}
    \sigma(z_i) = \frac{1}{1 + e^{-z_i}}
    \label{sigmoid}
\end{equation}
where $z_i = \sum_{k=1}^K w_{k,i}\alpha_{k} + b_i$. Here, $w_{i,k}$ is the weight between the $k$-th and $i$-th nodes in the second-to-last and output layer, respectively. $\alpha_k$ is the activation of the $k$-th node in the second-to-last layer, and $b_i$ is the bias value of the $i$-th node in the output layer. Note that the form of this activation function is identical to that of the ML2P model.


# Description of Exported Functions
There are two functions in this package which are available for the user. Both of them assist in constructing an ML2P-VAE model, but the two make different assumptions about the latent abilities. Each of these two functions return three Keras models: the encoder, decoder, and full VAE. This way, the user can train the VAE, use the ecoder to obtain ability parameter estimates from student's response sets, and use the decoder to obtain item parameter estimates.

## Independent Latent Traits
`build_vae_standard_normal` assumes that each of the latent abilities are independent of one another. Thus, the abilities $\Theta$ are distrubited according to $\mathcal{N}(0,I)$. Constructing this model yields a similar architecture to a typical VAE, which are often trained to a normal independent distribution. 

The encoder outputs $2\cdot($`num_skills`$)$ nodes, half of which represent the means of each latent trait, and the other half represent the log variance of each ability. There are no hidden layers in the decoder, and the encoded distribution is mapped directly to the output layer. The nonzero weights between these layers are determined by the Q-matrix.
```r
build_vae_standard_normal(num_items,
                          num_skills,
                          Q_matrix,
                          model_type = 2,
                          enc_hid_arch=c(ceiling((num_items + num_skills)/2)),
                          hid_enc_activations=rep('sigmoid', length(enc_hid_arch)),
                          output_activation='sigmoid',
                          kl_weight=1)
```                                      
The first three arguments must be provided by the user. `num_items` and `num_skills` describe the assessment length and the number of abilities being evaluated by the assessment. `Q_matrix` is a `num_skills` by `num_items` matrix which specifies the relationship between items and abilities. This matrix specifies the nonzero weights in the decoder of the VAE, which allows interpretation of network parameters as discrimination/difficulty parameter estimates and the hidden encoded layer as ability parameter estimates. Without the`Q_matrix`, estimation of these parameters would not be possible.

The other parameters in `build_vae_standard_normal` have default settings which can easily be adjusted. `model_type` can be either 1 or 2, specifying the use of a 1-Parameter or 2-Parameter Logistic model. If the 1-PL model is used, then all discrimination parameters (the weights in the decoder) are fixed to be equal to 1, and only the difficulty parameters (the biases in the output layer) are estimated.

`enc_hid_arch` is a vector of integers, who's length determines the number of hidden layers in the encoder, and the entries describe the number of nodes in the hidden layers. The default setting is to have one hidden layer in the encoder, with size halfway between `num_items` (the size of the input layer) and `num_skills` (the dimension of the encoded distribution). `hid_enc_activations` is a vector of strings, which specifies the activation functions to be used in each encoder hidden layer. Note that this must have the same length as `enc_hid_arch`, and each string must be a valid activation function supported by Tensorflow. Valid options include: 'elu', 'exponential', 'hard_sigmoid', 'linear', 'relu', 'selu', 'sigmoid', 'softmax', 'softplus', 'softsign', 'tanh'.

`output_activation` specifies the activation function in the output of the decoder. The default is to use a sigmoidal activation function, and should not be changed when using ML2P-VAE to estimate logistic models. If a different activation function is used, then the decoder cannot be interpreted as a ML2P model. The final option is the hyperparameter `kl_weight`, which changes the value of $\lambda$ in the VAE loss function
\[\mathcal{L}(w) = \mathcal{L}_{0}(w) + \lambda \mathcal{L}_{KL}(w),\]
where $\mathcal{L}_0$ is the reconstruction loss of the VAE, and $\mathcal{L}_{KL}$ is the KL-Divergence between $\mathcal{N}(0,I)$ and the distribution learned by the encoder.


## Correlated Latent Traits
The second model, `build_vae_normal_full_covariance`, allows the latent traits to be correlated with one another, i.e. that latent abilities $\Theta$ are distrubited acording to $\mathcal{N}(\mu, \Sigma)$. However, these relationships must be known, as the ML2P-VAE method does not estimate correlations between abilities. Thus, the full covariance/correlation matrix $\Sigma$ must be provided in addition to the student response sets and the Q-matrix.

The architecture of the decoder in `build_vae_normal_full_covariance` is exactly the same as that of `build_vae_standard_normal`. But the encoder outputs $K + K(K+1)/2$, where $K=$`num_skills`. The first $K$ represent the means of each latent ability, and the remaining $K(K+1)/2$ nodes form a lower triangular matrix $L$ which represents the matrix logarithm of the Cholesky decomposition of a covariance matrix. Thus for a data sample $x_0$, we can obtain $\mu_0$ and $L_0$, which we can use to construct $\Sigma_0 = e^{L_0} \cdot \left(e^{L_0}\right)^\top$. By construction, the matrix $\Sigma_0$ will always be symmetric and positive definite, regardless of the input $x_0$ or values of trainable parameters in the encoder. Thus $\Sigma_0$ can be interpreted as a covariance matrix, and the encoder maps each point $x_0$ to a multivariate Gaussian distribution $\mathcal{N}(\mu_0, \Sigma_0)$.
```r
build_vae_normal_full_covariance(num_items,
                                 num_skills,
                                 Q_matrix,
                                 model_type = 2,
                                 mean_vector = rep(0, num_skills),
                                 covariance_matrix = diag(num_skills),
                                 enc_hid_arch = c(ceiling((num_items + num_skills)/2)),
                                 hid_enc_activations = rep('sigmoid', length(enc_hid_arch)),
                                 output_activation = 'sigmoid',
                                 kl_weight = 1)
```
Every argument from `build_vae_standard_normal` is also included in `build_vae_normal_full_covariance`. There are two additional parameters, `mean_vector` and `covariance_matrix`, which specify $\mu$ and $\Sigma$ in the assumed multivariate Gaussian distribution of latent traits. `mean_vector` must be of length `num_skills`, and `covariance_matrix` must be a `num_skills` by `num_skills` symmetric positive definite matrix.

The default settings for these two arguments specify a $\mathcal{N}(0,I)$ distribution, but this method should never be used as such. Though it will work, `build_vae_standard_normal` will build a more efficient and more accurate model for assuming independent latent abilities. `build_vae_full_covariance` is most effectively used when these parameters specify a more compilcated distribution, such as $\mathcal{N}(\mu, \Sigma)$ or $\mathcal{N}(0, \Sigma)$.

# Examples
In this section, an example script is given showing how to construct, train, and evaluate an ML2P-VAE model. The data is from a simulated 30 item exam which assesses 3 latent traits. The latent abilities for 5000 students were sampled from $\mathcal{N}(0, \Sigma)$, where $\Sigma$ specifies the correlations between the 3 abilities. Discrimination and difficulty paramters were sampled uniformly from $[0.25, 1.75]$ and $[-3,3]$ respectively, and entries in the Q-matrix were sampled from Bern(0.35). All of these data files are included in the package. Probabilities for each student answering each question correctly were calculated with the ML2P model \cite{mckinley}. These probabilities were sampled from to generate a response to each item on the assessment for each student.

Two ML2P-VAE models are constructed. The first uses `build_vae_standard_normal`, and assumes that the latent traits are independent of on another. We know that for the data provided, this assumption is false - as the responses were generated dependent on the matrix $\Sigma$. But `build_vae_standard_normal` can be useful if the true correlations between latent traits are not known, i.e. `correlation_matrix` is not given. 

The second ML2P-VAE model makes stronger assumptions - that the exact relationship between abilities is known (and given). `build_vae_normal_full_covariance` encodes the response set to reflect the known correlations. On this data, it is unsurprising that this model yields better results than the first model, since it uses additional information about the distribution of the latent traits.

## A demo on constructing, training, and evaluating an ML2P-VAE model
```{r}
# Load data
data <- as.matrix(responses)
Q <- as.matrix(q_matrix)
cov <- as.matrix(correlation_matrix)

# Model parameters
num_items <- as.double(dim(Q)[2])
num_skills <- as.double(dim(Q)[1])
num_students <- dim(data)[1]
means <- rep(0,num_skills) 
enc_arch <- c(16L, 8L)
enc_act <- c('relu', 'tanh')
out_act <- 'sigmoid'
kl <- 1
```
```{r}
# Construct ML2P-VAE model assuming latent traits are independent
models_ind <- build_vae_standard_normal(num_items,
                                        num_skills,
                                        Q,
                                        model_type = 2,
                                        enc_hid_arch = enc_arch,
                                        hid_enc_activation = enc_act,
                                        output_activation = out_act)
encoder_ind <- models_ind[[1]]
decoder_ind <- models_ind[[2]]
vae_ind <- models_ind[[3]]
encoder_ind
decoder_ind
vae_ind
```
```{r}
# Construct ML2P-VAE model assuming correlation among traits is known
models_cor <- build_vae_normal_full_covariance(num_items,
                                           num_skills,
                                           Q,
                                           model_type = 2,
                                           mean_vector = means,
                                           covariance_matrix = cov,
                                           enc_hid_arch = enc_arch,
                                           hid_enc_activations = enc_act,
                                           output_activation = out_act,
                                           kl_weight = kl)
encoder_cor <- models_cor[[1]]
decoder_cor <- models_cor[[2]]
vae_cor <- models_cor[[3]]
encoder_cor
decoder_cor
vae_cor
```
```{r}
# Training parameters
num_train <- floor(0.8 * num_students)
num_test <- num_students - num_train
data_train <- data[1:num_train,]
data_test <- data[(num_train+1):num_students,]
num_epochs <- 12
batch_size <- 1
```
```{r}
# Train ML2P-VAE model assuming traits are independent
keras::fit(object = vae_ind,
           x = data_train,
           y = data_train,
           batch_size = batch_size,
           epochs = num_epochs,
           shuffle = FALSE,
           verbose = 2)
```
```{r}
# Train ML2P-VAE model assuming correlation among traits is known
keras::fit(object = vae_cor,
           x = data_train,
           y = data_train,
           batch_size = batch_size,
           epochs = num_epochs,
           shuffle = FALSE,
           verbose = 2)
```
```{r}
# Get parameter estimates for independent traits assumption
decoder_params_ind <- keras::get_weights(decoder_ind)
disc_est_ind <- decoder_params_ind[[1]]
diff_est_ind <- decoder_params_ind[[2]]
test_theta_est_ind <- predict(encoder_ind, data_test)[[1]]
all_theta_est_ind <- predict(encoder_ind, data)[[1]]
```
```{r}
# Get parameter estimates for correlated traits assumption
decoder_params_cor <- keras::get_weights(decoder_cor)
disc_est_cor <- decoder_params_cor[[1]]
diff_est_cor <- decoder_params_cor[[2]]
test_theta_est_cor <- predict(encoder_cor, data_test)[[1]]
all_theta_est_cor <- predict(encoder_cor, data)[[1]]
```
```{r}
# Load in true values
disc_true <- as.matrix(disc_true)
diff_true <- as.matrix(diff_true)
theta_true<- as.matrix(theta_true)
```

### Results assuming latent traits are independent

```{r}
# Examine ML2P-VAE model estimates (independent latent traits assumption)
par(pty="s")
matplot(t(disc_true), t(disc_est_ind), pch = '*',
        xlim = c(0.05, 2), ylim = c(0.05, 2),
        main = 'Discrimination Parameters',
        xlab = 'True', ylab = 'Estimated')
par(pty="s")
plot(diff_true, diff_est_ind, pch = '*',
     xlim = c(-3.1,3.1), ylim = c(-3.1,3.1),
     main = 'Difficulty Parameters',
     xlab = 'True', ylab = 'Estimated')
par(pty="s")
matplot(theta_true[4200:4400,], all_theta_est_ind[4200:4400,], pch = '*',
        xlim = c(-3,3), ylim = c(-3,3),
        main = 'Ability Parameters',
        xlab = 'True', ylab = 'Estimated')
```

### Results assuming latent traits are correlated

```{r}
# Examine ML2P-VAE model estimates (correlated latent traits assumption)
par(pty="s")
matplot(t(disc_true), t(disc_est_cor), pch = '*',
        xlim = c(0.05, 2), ylim = c(0.05, 2),
        main = 'Discrimination Parameters',
        xlab = 'True', ylab = 'Estimated')
par(pty="s")
plot(diff_true, diff_est_cor, pch = '*',
     xlim = c(-3.1,3.1), ylim = c(-3.1,3.1),
     main = 'Difficulty Parameters',
     xlab = 'True', ylab = 'Estimated')
par(pty="s")
matplot(theta_true[4200:4400,], all_theta_est_cor[4200:4400,], pch = '*',
        xlim = c(-3,3), ylim = c(-3,3),
        main = 'Ability Parameters',
        xlab = 'True', ylab = 'Estimated')
```



```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

\begin{thebibliography}{2}

\bibitem{ae_v_vae} Converse, Curi, Oliveira. ``Autoencoders for Educational Assessment.'' In proceedings of the Conference for Artificial Intelligence in Education (AIED), 2019.

\bibitem{curi2019} Curi, Converse, Hajewski, Oliveira. ``Interpretable Variational Autoencoders for Cognitive Models.'' In proceedings of the International Joint Conference on Neural Networks, 2019.

\bibitem{kingma} Kingma and Welling. ``Auto-Encoding Variational Bayes.'' In proceedings of The International Conference on Learning Representations, 2014.

\bibitem{mckinley} McKinley and Reckase. ``The use of the General Rasch Model with Multidimensional Item Response Data.'' American College Testing, 1980.

\bibitem{thissen} Thissen and Wainer ``Test Scoring''. Erlbaum Associates, Publishers, 2001.

\end{thebibliography}

