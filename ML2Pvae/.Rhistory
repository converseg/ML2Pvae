install.packages("tidyverse")
install.packages("tidyverse")
library(tidyverse)
install.packages("bindr")
library(tidyverse)
midwest
ggplot(data = midwest) +
geom_point(mapping = aes(x = popdensity, y = percollege))
ggplot(midwest) +
geom_point(aes(x = popdensity, y = percollege))
ggplot(midwest) +
geom_point(aes(x = popdensity, y = percollege, color = state))
ggplot(midwest) +
geom_point(aes(x = popdensity, y = state))
ggplot(midwest) +
geom_point(aes(x = state, y = pop_density))
ggplot(midwest) +
geom_point(aes(x = state, y = popdensity))
ggplot(midwest) +
geom_point(aes(x = county, y = popdensity))
ggplot(midwest) +
geom_point(aes(x = county, y = state))
ggplot(midwest) +
geom_point(aes(x = county, y = popdensity))
ggplot(midwest) +
geom_point(aes(x = popdensity, y = percollege, color = 'green'))
ggplot(midwest) +
geom_point(aes(x = popdensity, y = percollege), color = 'pink')
ggplot(midwest) +
geom_point(aes(x = popdensity, y = percollege), shape = 4)
ggplot(midwest) +
geom_point(aes(x = popdensity, y = percollege), shape = state)
ggplot(midwest) +
geom_point(aes(x = popdensity, y = percollege), shape = state))
ggplot(midwest) +
geom_point(aes(x = popdensity, y = percollege, shape = state))
ggplot(midwest) +
geom_point(aes(x = popdensity, y = percollege, color = state))
ggplot(midwest) +
geom_point(aes(x = popdensity, y = percollege, color = 'yellow'))
ggplot(midwest) +
geom_point(aes(x = popdensity, y = percollege, color = "yellow"))
geom_point(aes(x = popdensity, y = percollege, alpha = state)
ggplot(midwest) +
ggplot(midwest) +
geom_point(aes(x = popdensity, y = percollege, color = state))
ggplot(midwest) +
geom_point(aes(x = popdensity, y = percollege, color = state), alpha = .1)
ggplot(midwest) +
geom_point(aes(x = popdensity, y = percollege, color = state), alpha = .6)
p1 <- ggplot(midwest,
aes(x = popdensity, y = percollege, color = state)) +
geom_point() +
scale_x_continuous("Population Density",
breaks = seq(0, 80000, 20000)) +
scale_y_continuous("Percent College Graduates") +
theme_bw()
p1 +
labs(title = "Percent College Educated by Population Density",
subtitle = "County level data for five midwest states")
lotr <- read_tsv('https://raw.githubusercontent.com/jennybc/lotr/master/lotr_clean.tsv')
lotr
ggplot(lotr, aes(x = Words)) +
geom_histogram() +
theme_bw()
ggplot(lotr, aes(x = Words)) +
geom_histogram(bins = 20) +
theme_bw()
ggplot(lotr, aes(x = Words)) +
geom_histogram(binwidth = 25) +
theme_bw() +
facet_wrap(~ Film)
knitr::opts_chunk$set(fig.width=12, fig.height=6, fig.cap = NULL)
install.packages("plotly")
ggplot(midwest) +
geom_smooth(aes(x = popdensity, y = percollege, linetype = state),
se = FALSE)
ggplot(midwest) +
geom_point(aes(x = popdensity, y = percollege, color = state)) +
geom_smooth(aes(x = popdensity, y = percollege, color = state),
se = FALSE)
library(plotly)
library(tidyverse)
p <- ggplot(data = midwest) +
geom_point(mapping = aes(x = popdensity, y = percollege))
ggplotly(p)
p <- ggplot(midwest,
aes(x = popdensity, y = percollege, color = state)) +
geom_point() +
scale_x_continuous("Population Density",
breaks = seq(0, 80000, 20000)) +
scale_y_continuous("Percent College Graduates") +
scale_color_discrete("State") +
theme_bw()
ggplotly(p)
lotr <- read_tsv('https://raw.githubusercontent.com/jennybc/lotr/master/lotr_clean.tsv')
lotr
plot_ly(lotr, x = ~Words) %>% add_histogram()
one_plot <- function(d) {
plot_ly(d, x = ~Words) %>%
add_histogram() %>%
add_annotations(
~unique(Film), x = 0.5, y = 1,
xref = "paper", yref = "paper", showarrow = FALSE
)
}
lotr %>%
split(.$Film) %>%
lapply(one_plot) %>%
subplot(nrows = 1, shareX = TRUE, titleX = FALSE) %>%
hide_legend()
plot_ly(lotr, x = ~Race, color = ~Film) %>% add_histogram()
# number of diamonds by cut and clarity (n)
lotr_count <- count(lotr, Race, Film)
# number of diamonds by cut (nn)
lotr_prop <- left_join(lotr_count, count(lotr_count, Race, wt = n))
lotr_prop %>%
mutate(prop = n / nn) %>%
plot_ly(x = ~Race, y = ~prop, color = ~Film) %>%
add_bars() %>%
layout(barmode = "stack")
plot_ly(midwest, x = ~popdensity, y = ~percollege) %>%
add_markers()
hc <- highchart() %>%
hc_xAxis(categories = lotr_count$Race) %>%
hc_add_series(name = 'The Fellowship Of The Ring',
data = filter(lotr_count, Film == 'The Fellowship Of The Ring')$n) %>%
hc_add_series(name = 'The Two Towers',
data = filter(lotr_count, Film == 'The Two Towers')$n) %>%
hc_add_series(name = 'The Return Of The King',
data = filter(lotr_count, Film == 'The Return Of The King')$n)
library(highcharter)
devtools::install_github("jbkunst/highcharter")
ggplot(midwest) +
geom_point(aes(x = popdensity, y = percollege, color = state)) +
geom_smooth(aes(x = popdensity, y = percollege, color = state),
se = FALSE)
ggplot(midwest,
aes(x = popdensity, y = percollege, color = state)) +
geom_point() +
geom_smooth(se = FALSE)
ggplot(midwest) +
geom_smooth(aes(x = popdensity, y = percollege))
ggplot(midwest) +
geom_smooth(aes(x = popdensity, y = percollege, linetype = state),
se = FALSE)
source('~/.active-rstudio-document', echo=TRUE)
version
version
install.packages('installr')
check.for.updates.R()
library(installr)
check.for.updates.R()
updateR(F, T, F, F, F, F, T)
install.R()
version
updateR()
version
install.packages('tfprobability')
install.packages("tfprobability")
install.packages("tensorflow-probability")
library(tensorflow)
library(tfprobability)
tfe_enable_eager_execution()
install_tensorflow(conda="c:\programdata\anaconda3\lib\site-packages")
install_tensorflow(conda="c:/programdata/anaconda3/lib/site-packages")
install_tensorflow(conda="C:/programdata/anaconda3/lib/site-packages/tensorflow")
library(tensorflow)
install_tensorflow()
library(tensorflow)
library(tfprobability)
tfe_enable_eager_execution()
install.packages("tensorflow")
library(tensorflow)
install_tensorflow()
library(tensorflow)
library(tfprobability)
tfe_enable_eager_execution()
encoded_size <- 2
input_shape <- c(2L, 2L, 1L)
train_size <- 100
x_train <- array(runif(train_size * Reduce(`*`, input_shape)), dim = c(train_size, input_shape))
encoder_model <- keras_model_sequential() %>%
layer_flatten(input_shape = input_shape) %>%
layer_dense(units = 10, activation = "relu") %>%
layer_dense(units = params_size_multivariate_normal_tri_l(encoded_size)) %>%
layer_multivariate_normal_tri_l(event_size = encoded_size) %>%
# last layer adds KL divergence loss
layer_kl_divergence_add_loss(
distribution = tfd_independent(
tfd_normal(loc = c(0, 0), scale = 1),
reinterpreted_batch_ndims = 1
),
weight = train_size)
library(keras)
library(dplyr)
encoder_model <- keras_model_sequential() %>%
layer_flatten(input_shape = input_shape) %>%
layer_dense(units = 10, activation = "relu") %>%
layer_dense(units = params_size_multivariate_normal_tri_l(encoded_size)) %>%
layer_multivariate_normal_tri_l(event_size = encoded_size) %>%
# last layer adds KL divergence loss
layer_kl_divergence_add_loss(
distribution = tfd_independent(
tfd_normal(loc = c(0, 0), scale = 1),
reinterpreted_batch_ndims = 1
),
weight = train_size)
install_tensorflow(
extra_packages = c("keras", "tensorflow-hub", "tensorflow-probability"),
version = "1.12"
)
library(keras)
library(dplyr)
library(tensorflow)
library(tfprobability)
tfe_enable_eager_execution()
encoded_size <- 2
input_shape <- c(2L, 2L, 1L)
train_size <- 100
x_train <- array(runif(train_size * Reduce(`*`, input_shape)), dim = c(train_size, input_shape))
# encoder is a keras sequential model
encoder_model <- keras_model_sequential() %>%
layer_flatten(input_shape = input_shape) %>%
layer_dense(units = 10, activation = "relu") %>%
layer_dense(units = params_size_multivariate_normal_tri_l(encoded_size)) %>%
layer_multivariate_normal_tri_l(event_size = encoded_size) %>%
# last layer adds KL divergence loss
layer_kl_divergence_add_loss(
distribution = tfd_independent(
tfd_normal(loc = c(0, 0), scale = 1),
reinterpreted_batch_ndims = 1
),
weight = train_size)
# decoder is a keras sequential model
decoder_model <- keras_model_sequential() %>%
layer_dense(units = 10,
activation = 'relu',
input_shape = encoded_size) %>%
layer_dense(params_size_independent_bernoulli(input_shape)) %>%
layer_independent_bernoulli(event_shape = input_shape,
convert_to_tensor_fn = tfp$distributions$Bernoulli$logits)
# keras functional model uniting them both
vae_model <- keras_model(inputs = encoder_model$inputs,
outputs = decoder_model(encoder_model$outputs[1]))
# VAE loss now is just log probability of the data
vae_loss <- function (x, rv_x)
- (rv_x %>% tfd_log_prob(x))
vae_model %>% compile(
optimizer = tf$keras$optimizers$Adam(),
loss = vae_loss
)
vae_model %>% fit(x_train, x_train, batch_size = 25, epochs = 5)
library(tensorflow)
tfp <- import("tensorflow_probability")
tfd <- tfp$distributions
library(keras)
use_implementation("tensorflow")
library(tensorflow)
tfe_enable_eager_execution(device_policy = "silent")
library(glue)
np <- import("numpy")
kuzushiji <- np$load("kmnist-train-imgs.npz")
kuzushiji <- np$load("kmnist-train-imgs.npz")
library(devtools)
install('C:\\Users\\gconverse\\Desktop\\official_package\\ML2Pvae')
Q<- matrix(c(1,0,1,1,0,1,1,0), nrow = 2, ncol = 4)
models <- build_vae_standard_normal(4, 2, Q,
enc_hid_arch = c(6, 3), hid_enc_activation = c('sigmoid', 'relu'),
kl_weight = 0.1)
models <- ML2Pvae$build_vae_standard_normal(4, 2, Q,
enc_hid_arch = c(6, 3), hid_enc_activation = c('sigmoid', 'relu'),
kl_weight = 0.1)
?build_vae_standard_normal
??build_vae_standard_normal
library(ML2Pvae)
Q<- matrix(c(1,0,1,1,0,1,1,0), nrow = 2, ncol = 4)
models <- ML2Pvae$build_vae_standard_normal(4, 2, Q,
enc_hid_arch = c(6, 3), hid_enc_activation = c('sigmoid', 'relu'),
kl_weight = 0.1)
models <- build_vae_standard_normal(4, 2, Q,
enc_hid_arch = c(6, 3), hid_enc_activation = c('sigmoid', 'relu'),
kl_weight = 0.1)
models
?build_vae_standard_normal
?round
?ones
# data parameters
num_data <- 10000
num_items <- 28L
num_skills <- 3L
q_matrix <- matrix(rep(1,num_items*num_skills), nrow = num_skills, ncol = num_items)
View(q_matrix)
# training parameters
num_epochs <- 10
batch_size <- 64L
num_train <- floor(0.85 * num_data)
num_test <- num_data - num_train
kl_weight <- 1
diag(1, nrow=4, ncol=4)
# training parameters
num_epochs <- 10
batch_size <- 64L
num_train <- floor(0.85 * num_data)
num_test <- num_data - num_train
kl_weight <- 1
# If we're assuming independent latent abilities
encoder, decoder, vae <- build_vae_standard_normal(num_items, num_skills,
q_matrix, enc_hid_arch = c(15,10),
hid_enc_activations = c('sigmoid', 'sigmoid') kl_weight = 1)
# If we're assuming independent latent abilities
c(encoder, decoder, vae) <- build_vae_standard_normal(num_items, num_skills,
q_matrix, enc_hid_arch = c(15,10),
hid_enc_activations = c('sigmoid', 'sigmoid') kl_weight = 1)
# If we're assuming independent latent abilities
c(encoder, decoder, vae) <- build_vae_standard_normal(num_items, num_skills,
q_matrix, enc_hid_arch = c(15,10),
hid_enc_activations = c('sigmoid', 'sigmoid'), kl_weight = 1)
kl_weight <- 1L
# If we're assuming independent latent abilities
c(encoder, decoder, vae) <- build_vae_standard_normal(num_items, num_skills,
q_matrix, enc_hid_arch = c(15,10),
hid_enc_activations = c('sigmoid', 'sigmoid'), kl_weight = 1)
# If we're assuming independent latent abilities
c(encoder, decoder, vae) <- build_vae_standard_normal(num_items, num_skills,
q_matrix, enc_hid_arch = c(15L, 10L),
hid_enc_activations = c('sigmoid', 'sigmoid'), kl_weight = kl_weight)
q_matrix <- matrix(rep(1L,num_items*num_skills), nrow = num_skills, ncol = num_items)
# ability prior distribuion - assume mean 0 and variance 1
# only need this if we assume non-identity covariance matrix
skill_cov <- matrix(diag(1, nrow = num_skills, ncol = num_skills))
# training parameters
num_epochs <- 10
batch_size <- 64L
num_train <- floor(0.85 * num_data)
num_test <- num_data - num_train
kl_weight <- 1L
# If we're assuming independent latent abilities
c(encoder, decoder, vae) <- build_vae_standard_normal(num_items, num_skills,
q_matrix, enc_hid_arch = c(15L, 10L),
hid_enc_activations = c('sigmoid', 'sigmoid'), kl_weight = kl_weight)
# If we're assuming independent latent abilities
c(encoder, decoder, vae) <- build_vae_standard_normal(num_items, num_skills,
q_matrix)
# If we're assuming independent latent abilities
models <- build_vae_standard_normal(num_items, num_skills,
q_matrix, enc_hid_arch = c(15L, 10L),
hid_enc_activations = c('sigmoid', 'sigmoid'), kl_weight = kl_weight)
# If we're assuming independent latent abilities
models <- build_vae_standard_normal(num_items, num_skills,
q_matrix)
# data parameters
num_data <- 10000
num_items <- 28
num_skills <- 3
q_matrix <- matrix(rep(1L,num_items*num_skills), nrow = num_skills, ncol = num_items)
# ability prior distribuion - assume mean 0 and variance 1
# only need this if we assume non-identity covariance matrix
skill_cov <- matrix(diag(1, nrow = num_skills, ncol = num_skills))
# training parameters
num_epochs <- 10
batch_size <- 64L
num_train <- floor(0.85 * num_data)
num_test <- num_data - num_train
kl_weight <- 1L
# If we're assuming independent latent abilities
c(encoder, decoder, vae) <- build_vae_standard_normal(num_items, num_skills,
q_matrix, enc_hid_arch = c(15L, 10L),
hid_enc_activations = c('sigmoid', 'sigmoid'), kl_weight = kl_weight)
kl_weight <- 1
# If we're assuming independent latent abilities
c(encoder, decoder, vae) <- build_vae_standard_normal(num_items, num_skills,
q_matrix, enc_hid_arch = c(15L, 10L),
hid_enc_activations = c('sigmoid', 'sigmoid'), kl_weight = kl_weight)
# install('C:\\Users\\gconverse\\Desktop\\official_package\\ML2Pvae')
library(ML2Pvae)
# data parameters
num_data <- 10000
num_items <- 28
num_skills <- 3
q_matrix <- matrix(rep(1L,num_items*num_skills), nrow = num_skills, ncol = num_items)
# ability prior distribuion - assume mean 0 and variance 1
# only need this if we assume non-identity covariance matrix
skill_cov <- matrix(diag(1, nrow = num_skills, ncol = num_skills))
# training parameters
num_epochs <- 10
batch_size <- 64L
num_train <- floor(0.85 * num_data)
num_test <- num_data - num_train
kl_weight <- 1
# If we're assuming independent latent abilities
c(encoder, decoder, vae) <- build_vae_standard_normal(num_items, num_skills,
q_matrix, enc_hid_arch = c(15L, 10L),
hid_enc_activations = c('sigmoid', 'sigmoid'), kl_weight = kl_weight)
# If we're assuming independent latent abilities
models<- build_vae_standard_normal(num_items, num_skills,
q_matrix, enc_hid_arch = c(15L, 10L),
hid_enc_activations = c('sigmoid', 'sigmoid'), kl_weight = kl_weight)
# If we're assuming independent latent abilities
models <- build_vae_standard_normal(num_items, num_skills,
q_matrix, enc_hid_arch = c(15L, 10L),
hid_enc_activations = c('sigmoid', 'sigmoid'), kl_weight = kl_weight)
encoder <- models[1]
encoder
encoder <- models[[1]]
encoder
decoder <- models[[2]]
vae <- models[[3]]
vae
# If we assume correlation between latent abilities
models <- build_vae_normal_full_covariance(num_items, nnum_skills,
q_matrix, covariance_matrix = skill_cov,
enc_hid_arch = c(15L, 10L),
hid_enc_activations = c('sigmoid', 'sigmoid'),
kl_weight = kl_weight)
# ability prior distribuion - assume mean 0 and variance 1
# only need this if we assume non-identity covariance matrix
skill_cov <- matrix(diag(1, nrow = num_skills, ncol = num_skills))
# If we assume correlation between latent abilities
models <- build_vae_normal_full_covariance(num_items, nnum_skills,
q_matrix, covariance_matrix = skill_cov,
enc_hid_arch = c(15L, 10L),
hid_enc_activations = c('sigmoid', 'sigmoid'),
kl_weight = kl_weight)
skill_cov
# ability prior distribuion - assume mean 0 and variance 1
# only need this if we assume non-identity covariance matrix
skill_cov <- diag(1, nrow = num_skills, ncol = num_skills)
skill_cov
# If we assume correlation between latent abilities
models <- build_vae_normal_full_covariance(num_items, nnum_skills,
q_matrix, covariance_matrix = skill_cov,
enc_hid_arch = c(15L, 10L),
hid_enc_activations = c('sigmoid', 'sigmoid'),
kl_weight = kl_weight)
# If we assume correlation between latent abilities
models <- build_vae_normal_full_covariance(num_items, num_skills,
q_matrix, covariance_matrix = skill_cov,
enc_hid_arch = c(15L, 10L),
hid_enc_activations = c('sigmoid', 'sigmoid'),
kl_weight = kl_weight)
encoder <- models[[1]]
decoder <- models[[2]]
vae <- models[[3]]
encoder
?norm
norm(q_matrix)
norm(q_matrix, 2)
norm(q_matrix, type='1')
norm(q_matrix, type='2')
norm(q_matrix, type='F')
q_matrix
sum(q_matrix)
setwd("C:/Users/gconverse/Desktop/ML2Pvae_repo/ML2Pvae/ML2Pvae")
library(devtools)
document()
test()
tfe_enable_eager_execution
library(keras)
library(tensorflow)
tfe_enable_eager_execution()
